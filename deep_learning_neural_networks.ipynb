{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant(5)\n",
    "x2 = tf.constant(6)\n",
    "\n",
    "result = tf.multiply(x1,x2)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(result))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput > weight > hidden layer 1 (activation function)  > weights > hidden l 2 (activation function) > weights > \\noutput layer\\n\\ncompare output to intended output > cost function (cross entropy)\\noptimization function(optimizer) > minimize cost (AdamOptimizer....SGD, AdaGrad)\\n\\nbackpropogation\\n\\nfeed forward+backprop = epoch\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "input > weight > hidden layer 1 (activation function)  > weights > hidden l 2 (activation function) > weights > \n",
    "output layer\n",
    "\n",
    "compare output to intended output > cost function (cross entropy)\n",
    "optimization function(optimizer) > minimize cost (AdamOptimizer....SGD, AdaGrad)\n",
    "\n",
    "backpropogation\n",
    "\n",
    "feed forward+backprop = epoch\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 completed out of 20 loss: 585853.8907318115\n",
      "Epoch 1 completed out of 20 loss: 126639.99866867065\n",
      "Epoch 2 completed out of 20 loss: 75780.45319366455\n",
      "Epoch 3 completed out of 20 loss: 51372.74943470955\n",
      "Epoch 4 completed out of 20 loss: 36072.39020574093\n",
      "Epoch 5 completed out of 20 loss: 25825.30467939377\n",
      "Epoch 6 completed out of 20 loss: 18712.25850987315\n",
      "Epoch 7 completed out of 20 loss: 13735.063741649441\n",
      "Epoch 8 completed out of 20 loss: 10257.865893954175\n",
      "Epoch 9 completed out of 20 loss: 7403.767608346028\n",
      "Epoch 10 completed out of 20 loss: 5391.57618905096\n",
      "Epoch 11 completed out of 20 loss: 4413.210320613318\n",
      "Epoch 12 completed out of 20 loss: 3397.028035644183\n",
      "Epoch 13 completed out of 20 loss: 2724.9573218645965\n",
      "Epoch 14 completed out of 20 loss: 2611.7317064917406\n",
      "Epoch 15 completed out of 20 loss: 1771.4967268177302\n",
      "Epoch 16 completed out of 20 loss: 1761.9562123930593\n",
      "Epoch 17 completed out of 20 loss: 1589.772938263579\n",
      "Epoch 18 completed out of 20 loss: 1475.4059888170716\n",
      "Epoch 19 completed out of 20 loss: 1298.2228346429017\n",
      "Accuracy: 0.9465\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "n_nodes_hl1 = 300\n",
    "n_nodes_hl2 = 200\n",
    "n_nodes_hl3 = 100\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "#height x width\n",
    "x = tf.placeholder('float',[None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "        # (input data * weights) + biases\n",
    "        \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(  tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    #      learning_rate_default = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    #cycles feed forward + backprop\n",
    "    hm_epochs = 20\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction,1 ), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "train_neural_network(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/frank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/frank/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 10000000\n",
    "\n",
    "def create_lexicon(pos,neg):\n",
    "    lexicon = []\n",
    "    for fi in [pos,neg]:\n",
    "        with open(fi, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            for l in contents[:hm_lines]:\n",
    "                all_words = word_tokenize(l.lower())\n",
    "                lexicon += list(all_words)\n",
    "                \n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    l2 = []\n",
    "    for w in w_counts:\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            l2.append(w)\n",
    "    print(len(l2))\n",
    "    return l2\n",
    "\n",
    "def sample_handling(sample, lexicon, classification):\n",
    "    featureset=[]\n",
    "    \n",
    "    with open(sample, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(l.lower())\n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    features[index_value] += 1\n",
    "                features = list(features)\n",
    "                featureset.append([features, classification])\n",
    "                \n",
    "    return featureset\n",
    "\n",
    "def create_feature_sets_and_labels(pos,neg,test_size=0.1):\n",
    "    lexicon = create_lexicon(pos,neg)\n",
    "    features = []\n",
    "    features += sample_handling('pos.txt', lexicon,[1,0])\n",
    "    features += sample_handling('neg.txt', lexicon,[0,1])\n",
    "    random.shuffle(features)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    testing_size = int(test_size*len(features))\n",
    "    \n",
    "    train_x = list(features[:,0][:-testing_size])\n",
    "    train_y = list(features[:,1][:-testing_size])\n",
    "    \n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    test_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n",
    "    with open('sentiment_set.pickle', 'wb') as f:\n",
    "        pickle.dump([train_x,train_y,test_x,test_y], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "Epoch 0 completed out of 20 loss: 730503.8408737183\n",
      "Epoch 1 completed out of 20 loss: 111575.7743692398\n",
      "Epoch 2 completed out of 20 loss: 62349.23623275757\n",
      "Epoch 3 completed out of 20 loss: 50111.205814540386\n",
      "Epoch 4 completed out of 20 loss: 43829.947853565216\n",
      "Epoch 5 completed out of 20 loss: 39830.41049528122\n",
      "Epoch 6 completed out of 20 loss: 34853.34167611599\n",
      "Epoch 7 completed out of 20 loss: 31793.373434901237\n",
      "Epoch 8 completed out of 20 loss: 28193.914291620255\n",
      "Epoch 9 completed out of 20 loss: 24710.750375539064\n",
      "Epoch 10 completed out of 20 loss: 22081.29998409748\n",
      "Epoch 11 completed out of 20 loss: 19765.11409842968\n",
      "Epoch 12 completed out of 20 loss: 17627.389706641436\n",
      "Epoch 13 completed out of 20 loss: 15594.471472531557\n",
      "Epoch 14 completed out of 20 loss: 14147.026262104511\n",
      "Epoch 15 completed out of 20 loss: 12184.336075142026\n",
      "Epoch 16 completed out of 20 loss: 10818.940199971199\n",
      "Epoch 17 completed out of 20 loss: 9757.307924956083\n",
      "Epoch 18 completed out of 20 loss: 8684.186857275665\n",
      "Epoch 19 completed out of 20 loss: 7919.296570152044\n",
      "Accuracy: 0.86849993\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from deep_learning_neural_networks import create_feature_sets_and_labels\n",
    "import numpy as np\n",
    "\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "\n",
    "#height x width\n",
    "x = tf.placeholder('float',[None, len(train_x[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "        # (input data * weights) + biases\n",
    "        \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(  tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    #      learning_rate_default = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    #cycles feed forward + backprop\n",
    "    hm_epochs = 20\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i \n",
    "                end = i + batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "                epoch_loss += c\n",
    "                \n",
    "                i += batch_size\n",
    "            print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction,1 ), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:test_x, y:test_y}))\n",
    "        \n",
    "train_neural_network(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 26\n",
      "5000 32\n",
      "7500 41\n",
      "10000 63\n",
      "12500 85\n",
      "15000 89\n",
      "17500 91\n",
      "20000 108\n",
      "22500 113\n",
      "25000 120\n",
      "27500 128\n",
      "30000 141\n",
      "32500 159\n",
      "35000 173\n",
      "37500 177\n",
      "40000 185\n",
      "42500 198\n",
      "45000 212\n",
      "47500 224\n",
      "50000 240\n",
      "52500 253\n",
      "55000 266\n",
      "57500 267\n",
      "60000 269\n",
      "62500 280\n",
      "65000 284\n",
      "67500 291\n",
      "70000 299\n",
      "72500 306\n",
      "75000 321\n",
      "77500 329\n",
      "80000 336\n",
      "82500 345\n",
      "85000 356\n",
      "87500 358\n",
      "90000 361\n",
      "92500 374\n",
      "95000 388\n",
      "97500 399\n",
      "100000 402\n",
      "102500 412\n",
      "105000 418\n",
      "107500 425\n",
      "110000 429\n",
      "112500 438\n",
      "115000 448\n",
      "117500 450\n",
      "120000 453\n",
      "122500 460\n",
      "125000 465\n",
      "127500 466\n",
      "130000 468\n",
      "132500 470\n",
      "135000 480\n",
      "137500 490\n",
      "140000 496\n",
      "142500 500\n",
      "145000 507\n",
      "147500 514\n",
      "150000 528\n",
      "152500 532\n",
      "155000 536\n",
      "157500 540\n",
      "160000 548\n",
      "162500 556\n",
      "165000 560\n",
      "167500 562\n",
      "170000 566\n",
      "172500 571\n",
      "175000 580\n",
      "177500 583\n",
      "180000 588\n",
      "182500 594\n",
      "185000 597\n",
      "187500 600\n",
      "190000 607\n",
      "192500 611\n",
      "195000 621\n",
      "197500 624\n",
      "200000 628\n",
      "202500 630\n",
      "205000 632\n",
      "207500 641\n",
      "210000 644\n",
      "212500 648\n",
      "215000 651\n",
      "217500 659\n",
      "220000 674\n",
      "222500 684\n",
      "225000 689\n",
      "227500 694\n",
      "230000 698\n",
      "232500 700\n",
      "235000 706\n",
      "237500 714\n",
      "240000 717\n",
      "242500 721\n",
      "245000 722\n",
      "247500 725\n",
      "250000 727\n",
      "252500 732\n",
      "255000 741\n",
      "257500 743\n",
      "260000 747\n",
      "262500 754\n",
      "265000 757\n",
      "267500 762\n",
      "270000 765\n",
      "272500 767\n",
      "275000 771\n",
      "277500 776\n",
      "280000 776\n",
      "282500 777\n",
      "285000 781\n",
      "287500 782\n",
      "290000 786\n",
      "292500 793\n",
      "295000 797\n",
      "297500 799\n",
      "300000 805\n",
      "302500 808\n",
      "305000 812\n",
      "307500 815\n",
      "310000 818\n",
      "312500 820\n",
      "315000 825\n",
      "317500 828\n",
      "320000 830\n",
      "322500 833\n",
      "325000 834\n",
      "327500 844\n",
      "330000 848\n",
      "332500 852\n",
      "335000 855\n",
      "337500 859\n",
      "340000 861\n",
      "342500 864\n",
      "345000 870\n",
      "347500 879\n",
      "350000 881\n",
      "352500 892\n",
      "355000 895\n",
      "357500 903\n",
      "360000 906\n",
      "362500 912\n",
      "365000 915\n",
      "367500 916\n",
      "370000 924\n",
      "372500 937\n",
      "375000 941\n",
      "377500 945\n",
      "380000 947\n",
      "382500 948\n",
      "385000 952\n",
      "387500 956\n",
      "390000 964\n",
      "392500 966\n",
      "395000 972\n",
      "397500 980\n",
      "400000 982\n",
      "402500 989\n",
      "405000 990\n",
      "407500 993\n",
      "410000 996\n",
      "412500 998\n",
      "415000 1000\n",
      "417500 1006\n",
      "420000 1012\n",
      "422500 1014\n",
      "425000 1017\n",
      "427500 1023\n",
      "430000 1025\n",
      "432500 1027\n",
      "435000 1032\n",
      "437500 1035\n",
      "440000 1039\n",
      "442500 1044\n",
      "445000 1046\n",
      "447500 1048\n",
      "450000 1051\n",
      "452500 1061\n",
      "455000 1063\n",
      "457500 1064\n",
      "460000 1069\n",
      "462500 1077\n",
      "465000 1078\n",
      "467500 1080\n",
      "470000 1082\n",
      "472500 1085\n",
      "475000 1089\n",
      "477500 1092\n",
      "480000 1093\n",
      "482500 1102\n",
      "485000 1106\n",
      "487500 1109\n",
      "490000 1113\n",
      "492500 1120\n",
      "495000 1124\n",
      "497500 1126\n",
      "500000 1128\n",
      "502500 1134\n",
      "505000 1135\n",
      "507500 1147\n",
      "510000 1148\n",
      "512500 1151\n",
      "515000 1159\n",
      "517500 1162\n",
      "520000 1166\n",
      "522500 1168\n",
      "525000 1170\n",
      "527500 1176\n",
      "530000 1178\n",
      "532500 1186\n",
      "535000 1194\n",
      "537500 1196\n",
      "540000 1201\n",
      "542500 1202\n",
      "545000 1204\n",
      "547500 1209\n",
      "550000 1215\n",
      "552500 1218\n",
      "555000 1221\n",
      "557500 1225\n",
      "560000 1229\n",
      "562500 1233\n",
      "565000 1236\n",
      "567500 1238\n",
      "570000 1240\n",
      "572500 1240\n",
      "575000 1245\n",
      "577500 1251\n",
      "580000 1260\n",
      "582500 1264\n",
      "585000 1266\n",
      "587500 1275\n",
      "590000 1276\n",
      "592500 1278\n",
      "595000 1279\n",
      "597500 1285\n",
      "600000 1290\n",
      "602500 1296\n",
      "605000 1299\n",
      "607500 1303\n",
      "610000 1303\n",
      "612500 1309\n",
      "615000 1316\n",
      "617500 1317\n",
      "620000 1320\n",
      "622500 1321\n",
      "625000 1323\n",
      "627500 1323\n",
      "630000 1324\n",
      "632500 1329\n",
      "635000 1333\n",
      "637500 1335\n",
      "640000 1337\n",
      "642500 1337\n",
      "645000 1340\n",
      "647500 1344\n",
      "650000 1344\n",
      "652500 1351\n",
      "655000 1355\n",
      "657500 1357\n",
      "660000 1357\n",
      "662500 1360\n",
      "665000 1362\n",
      "667500 1369\n",
      "670000 1377\n",
      "672500 1384\n",
      "675000 1392\n",
      "677500 1397\n",
      "680000 1407\n",
      "682500 1409\n",
      "685000 1412\n",
      "687500 1414\n",
      "690000 1422\n",
      "692500 1428\n",
      "695000 1434\n",
      "697500 1437\n",
      "700000 1439\n",
      "702500 1442\n",
      "705000 1448\n",
      "707500 1455\n",
      "710000 1463\n",
      "712500 1473\n",
      "715000 1480\n",
      "717500 1482\n",
      "720000 1494\n",
      "722500 1498\n",
      "725000 1504\n",
      "727500 1512\n",
      "730000 1518\n",
      "732500 1522\n",
      "735000 1523\n",
      "737500 1524\n",
      "740000 1526\n",
      "742500 1527\n",
      "745000 1531\n",
      "747500 1537\n",
      "750000 1542\n",
      "752500 1547\n",
      "755000 1548\n",
      "757500 1550\n",
      "760000 1555\n",
      "762500 1559\n",
      "765000 1562\n",
      "767500 1567\n",
      "770000 1569\n",
      "772500 1570\n",
      "775000 1575\n",
      "777500 1577\n",
      "780000 1579\n",
      "782500 1588\n",
      "785000 1590\n",
      "787500 1594\n",
      "790000 1596\n",
      "792500 1596\n",
      "795000 1598\n",
      "797500 1603\n",
      "800000 1608\n",
      "802500 1615\n",
      "805000 1617\n",
      "807500 1622\n",
      "810000 1626\n",
      "812500 1629\n",
      "815000 1633\n",
      "817500 1634\n",
      "820000 1644\n",
      "822500 1646\n",
      "825000 1652\n",
      "827500 1655\n",
      "830000 1657\n",
      "832500 1659\n",
      "835000 1660\n",
      "837500 1663\n",
      "840000 1664\n",
      "842500 1665\n",
      "845000 1665\n",
      "847500 1666\n",
      "850000 1679\n",
      "852500 1685\n",
      "855000 1689\n",
      "857500 1693\n",
      "860000 1695\n",
      "862500 1700\n",
      "865000 1701\n",
      "867500 1703\n",
      "870000 1704\n",
      "872500 1710\n",
      "875000 1714\n",
      "877500 1715\n",
      "880000 1716\n",
      "882500 1718\n",
      "885000 1721\n",
      "887500 1725\n",
      "890000 1727\n",
      "892500 1731\n",
      "895000 1735\n",
      "897500 1742\n",
      "900000 1742\n",
      "902500 1746\n",
      "905000 1749\n",
      "907500 1750\n",
      "910000 1751\n",
      "912500 1758\n",
      "915000 1761\n",
      "917500 1764\n",
      "920000 1778\n",
      "922500 1781\n",
      "925000 1787\n",
      "927500 1799\n",
      "930000 1803\n",
      "932500 1806\n",
      "935000 1807\n",
      "937500 1809\n",
      "940000 1810\n",
      "942500 1813\n",
      "945000 1819\n",
      "947500 1820\n",
      "950000 1820\n",
      "952500 1821\n",
      "955000 1823\n",
      "957500 1830\n",
      "960000 1836\n",
      "962500 1840\n",
      "965000 1842\n",
      "967500 1847\n",
      "970000 1852\n",
      "972500 1859\n",
      "975000 1861\n",
      "977500 1870\n",
      "980000 1874\n",
      "982500 1876\n",
      "985000 1878\n",
      "987500 1879\n",
      "990000 1886\n",
      "992500 1888\n",
      "995000 1895\n",
      "997500 1896\n",
      "1000000 1899\n",
      "1002500 1900\n",
      "1005000 1901\n",
      "1007500 1913\n",
      "1010000 1915\n",
      "1012500 1918\n",
      "1015000 1922\n",
      "1017500 1926\n",
      "1020000 1928\n",
      "1022500 1930\n",
      "1025000 1931\n",
      "1027500 1932\n",
      "1030000 1939\n",
      "1032500 1942\n",
      "1035000 1944\n",
      "1037500 1949\n",
      "1040000 1953\n",
      "1042500 1958\n",
      "1045000 1961\n",
      "1047500 1970\n",
      "1050000 1972\n",
      "1052500 1975\n",
      "1055000 1978\n",
      "1057500 1978\n",
      "1060000 1980\n",
      "1062500 1986\n",
      "1065000 1988\n",
      "1067500 1990\n",
      "1070000 1994\n",
      "1072500 1995\n",
      "1075000 1997\n",
      "1077500 1997\n",
      "1080000 1997\n",
      "1082500 2000\n",
      "1085000 2007\n",
      "1087500 2017\n",
      "1090000 2020\n",
      "1092500 2024\n",
      "1095000 2028\n",
      "1097500 2032\n",
      "1100000 2038\n",
      "1102500 2043\n",
      "1105000 2048\n",
      "1107500 2054\n",
      "1110000 2057\n",
      "1112500 2058\n",
      "1115000 2060\n",
      "1117500 2065\n",
      "1120000 2068\n",
      "1122500 2072\n",
      "1125000 2080\n",
      "1127500 2086\n",
      "1130000 2087\n",
      "1132500 2089\n",
      "1135000 2089\n",
      "1137500 2094\n",
      "1140000 2099\n",
      "1142500 2101\n",
      "1145000 2105\n",
      "1147500 2109\n",
      "1150000 2113\n",
      "1152500 2114\n",
      "1155000 2120\n",
      "1157500 2124\n",
      "1160000 2126\n",
      "1162500 2132\n",
      "1165000 2133\n",
      "1167500 2146\n",
      "1170000 2153\n",
      "1172500 2159\n",
      "1175000 2161\n",
      "1177500 2164\n",
      "1180000 2164\n",
      "1182500 2165\n",
      "1185000 2167\n",
      "1187500 2167\n",
      "1190000 2174\n",
      "1192500 2178\n",
      "1195000 2181\n",
      "1197500 2185\n",
      "1200000 2186\n",
      "1202500 2190\n",
      "1205000 2196\n",
      "1207500 2197\n",
      "1210000 2199\n",
      "1212500 2202\n",
      "1215000 2205\n",
      "1217500 2205\n",
      "1220000 2212\n",
      "1222500 2216\n",
      "1225000 2219\n",
      "1227500 2223\n",
      "1230000 2225\n",
      "1232500 2228\n",
      "1235000 2229\n",
      "1237500 2229\n",
      "1240000 2231\n",
      "1242500 2233\n",
      "1245000 2233\n",
      "1247500 2234\n",
      "1250000 2237\n",
      "1252500 2239\n",
      "1255000 2242\n",
      "1257500 2246\n",
      "1260000 2250\n",
      "1262500 2254\n",
      "1265000 2257\n",
      "1267500 2260\n",
      "1270000 2261\n",
      "1272500 2261\n",
      "1275000 2262\n",
      "1277500 2265\n",
      "1280000 2272\n",
      "1282500 2273\n",
      "1285000 2273\n",
      "1287500 2277\n",
      "1290000 2281\n",
      "1292500 2284\n",
      "1295000 2285\n",
      "1297500 2286\n",
      "1300000 2287\n",
      "1302500 2289\n",
      "1305000 2291\n",
      "1307500 2293\n",
      "1310000 2297\n",
      "1312500 2297\n",
      "1315000 2299\n",
      "1317500 2303\n",
      "1320000 2304\n",
      "1322500 2306\n",
      "1325000 2310\n",
      "1327500 2314\n",
      "1330000 2318\n",
      "1332500 2323\n",
      "1335000 2327\n",
      "1337500 2327\n",
      "1340000 2330\n",
      "1342500 2330\n",
      "1345000 2335\n",
      "1347500 2339\n",
      "1350000 2344\n",
      "1352500 2345\n",
      "1355000 2345\n",
      "1357500 2348\n",
      "1360000 2350\n",
      "1362500 2351\n",
      "1365000 2356\n",
      "1367500 2358\n",
      "1370000 2361\n",
      "1372500 2364\n",
      "1375000 2368\n",
      "1377500 2370\n",
      "1380000 2374\n",
      "1382500 2375\n",
      "1385000 2376\n",
      "1387500 2380\n",
      "1390000 2384\n",
      "1392500 2386\n",
      "1395000 2387\n",
      "1397500 2390\n",
      "1400000 2393\n",
      "1402500 2397\n",
      "1405000 2397\n",
      "1407500 2398\n",
      "1410000 2404\n",
      "1412500 2407\n",
      "1415000 2413\n",
      "1417500 2414\n",
      "1420000 2416\n",
      "1422500 2421\n",
      "1425000 2422\n",
      "1427500 2423\n",
      "1430000 2426\n",
      "1432500 2427\n",
      "1435000 2428\n",
      "1437500 2431\n",
      "1440000 2432\n",
      "1442500 2435\n",
      "1445000 2438\n",
      "1447500 2438\n",
      "1450000 2440\n",
      "1452500 2443\n",
      "1455000 2446\n",
      "1457500 2452\n",
      "1460000 2455\n",
      "1462500 2456\n",
      "1465000 2458\n",
      "1467500 2459\n",
      "1470000 2468\n",
      "1472500 2470\n",
      "1475000 2473\n",
      "1477500 2480\n",
      "1480000 2483\n",
      "1482500 2491\n",
      "1485000 2498\n",
      "1487500 2499\n",
      "1490000 2500\n",
      "1492500 2501\n",
      "1495000 2504\n",
      "1497500 2509\n",
      "1500000 2513\n",
      "1502500 2515\n",
      "1505000 2516\n",
      "1507500 2517\n",
      "1510000 2518\n",
      "1512500 2523\n",
      "1515000 2526\n",
      "1517500 2528\n",
      "1520000 2529\n",
      "1522500 2531\n",
      "1525000 2536\n",
      "1527500 2542\n",
      "1530000 2543\n",
      "1532500 2553\n",
      "1535000 2555\n",
      "1537500 2558\n",
      "1540000 2561\n",
      "1542500 2562\n",
      "1545000 2564\n",
      "1547500 2571\n",
      "1550000 2573\n",
      "1552500 2574\n",
      "1555000 2579\n",
      "1557500 2579\n",
      "1560000 2582\n",
      "1562500 2588\n",
      "1565000 2590\n",
      "1567500 2591\n",
      "1570000 2595\n",
      "1572500 2599\n",
      "1575000 2601\n",
      "1577500 2603\n",
      "1580000 2605\n",
      "1582500 2606\n",
      "1585000 2609\n",
      "1587500 2614\n",
      "1590000 2618\n",
      "1592500 2625\n",
      "1595000 2626\n",
      "1597500 2632\n",
      "1600000 2638\n",
      "1602500 2638\n",
      "1605000 2638\n",
      "1607500 2638\n",
      "1610000 2638\n",
      "1612500 2638\n",
      "1615000 2638\n",
      "1617500 2638\n",
      "1620000 2638\n",
      "1622500 2638\n",
      "1625000 2638\n",
      "1627500 2638\n",
      "1630000 2638\n",
      "1632500 2638\n",
      "1635000 2638\n",
      "1637500 2638\n",
      "1640000 2638\n",
      "1642500 2638\n",
      "1645000 2638\n",
      "1647500 2638\n",
      "1650000 2638\n",
      "1652500 2638\n",
      "1655000 2638\n",
      "1657500 2638\n",
      "1660000 2638\n",
      "1662500 2638\n",
      "1665000 2638\n",
      "1667500 2638\n",
      "1670000 2638\n",
      "1672500 2638\n",
      "1675000 2638\n",
      "1677500 2638\n",
      "1680000 2638\n",
      "1682500 2638\n",
      "1685000 2638\n",
      "1687500 2638\n",
      "1690000 2638\n",
      "1692500 2638\n",
      "1695000 2638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1697500 2638\n",
      "1700000 2638\n",
      "1702500 2638\n",
      "1705000 2638\n",
      "1707500 2638\n",
      "1710000 2638\n",
      "1712500 2638\n",
      "1715000 2638\n",
      "1717500 2638\n",
      "1720000 2638\n",
      "1722500 2638\n",
      "1725000 2638\n",
      "1727500 2638\n",
      "1730000 2638\n",
      "1732500 2638\n",
      "1735000 2638\n",
      "1737500 2638\n",
      "1740000 2638\n",
      "1742500 2638\n",
      "1745000 2638\n",
      "1747500 2638\n",
      "1750000 2638\n",
      "1752500 2638\n",
      "1755000 2638\n",
      "1757500 2638\n",
      "1760000 2638\n",
      "1762500 2638\n",
      "1765000 2638\n",
      "1767500 2638\n",
      "1770000 2638\n",
      "1772500 2638\n",
      "1775000 2638\n",
      "1777500 2638\n",
      "1780000 2638\n",
      "1782500 2638\n",
      "1785000 2638\n",
      "1787500 2638\n",
      "1790000 2638\n",
      "1792500 2638\n",
      "1795000 2638\n",
      "1797500 2638\n",
      "1800000 2638\n",
      "1802500 2638\n",
      "1805000 2638\n",
      "1807500 2638\n",
      "1810000 2638\n",
      "1812500 2638\n",
      "1815000 2638\n",
      "1817500 2638\n",
      "1820000 2638\n",
      "1822500 2638\n",
      "1825000 2638\n",
      "1827500 2638\n",
      "1830000 2638\n",
      "1832500 2638\n",
      "1835000 2638\n",
      "1837500 2638\n",
      "1840000 2638\n",
      "1842500 2638\n",
      "1845000 2638\n",
      "1847500 2638\n",
      "1850000 2638\n",
      "1852500 2638\n",
      "1855000 2638\n",
      "1857500 2638\n",
      "1860000 2638\n",
      "1862500 2638\n",
      "1865000 2638\n",
      "1867500 2638\n",
      "1870000 2638\n",
      "1872500 2638\n",
      "1875000 2638\n",
      "1877500 2638\n",
      "1880000 2638\n",
      "1882500 2638\n",
      "1885000 2638\n",
      "1887500 2638\n",
      "1890000 2638\n",
      "1892500 2638\n",
      "1895000 2638\n",
      "1897500 2638\n",
      "1900000 2638\n",
      "1902500 2638\n",
      "1905000 2638\n",
      "1907500 2638\n",
      "1910000 2638\n",
      "1912500 2638\n",
      "1915000 2638\n",
      "1917500 2638\n",
      "1920000 2638\n",
      "1922500 2638\n",
      "1925000 2638\n",
      "1927500 2638\n",
      "1930000 2638\n",
      "1932500 2638\n",
      "1935000 2638\n",
      "1937500 2638\n",
      "1940000 2638\n",
      "1942500 2638\n",
      "1945000 2638\n",
      "1947500 2638\n",
      "1950000 2638\n",
      "1952500 2638\n",
      "1955000 2638\n",
      "1957500 2638\n",
      "1960000 2638\n",
      "1962500 2638\n",
      "1965000 2638\n",
      "1967500 2638\n",
      "1970000 2638\n",
      "1972500 2638\n",
      "1975000 2638\n",
      "1977500 2638\n",
      "1980000 2638\n",
      "1982500 2638\n",
      "1985000 2638\n",
      "1987500 2638\n",
      "1990000 2638\n",
      "1992500 2638\n",
      "1995000 2638\n",
      "1997500 2638\n",
      "2000000 2638\n",
      "2002500 2638\n",
      "2005000 2638\n",
      "2007500 2638\n",
      "2010000 2638\n",
      "2012500 2638\n",
      "2015000 2638\n",
      "2017500 2638\n",
      "2020000 2638\n",
      "2022500 2638\n",
      "2025000 2638\n",
      "2027500 2638\n",
      "2030000 2638\n",
      "2032500 2638\n",
      "2035000 2638\n",
      "2037500 2638\n",
      "2040000 2638\n",
      "2042500 2638\n",
      "2045000 2638\n",
      "2047500 2638\n",
      "2050000 2638\n",
      "2052500 2638\n",
      "2055000 2638\n",
      "2057500 2638\n",
      "2060000 2638\n",
      "2062500 2638\n",
      "2065000 2638\n",
      "2067500 2638\n",
      "2070000 2638\n",
      "2072500 2638\n",
      "2075000 2638\n",
      "2077500 2638\n",
      "2080000 2638\n",
      "2082500 2638\n",
      "2085000 2638\n",
      "2087500 2638\n",
      "2090000 2638\n",
      "2092500 2638\n",
      "2095000 2638\n",
      "2097500 2638\n",
      "2100000 2638\n",
      "2102500 2638\n",
      "2105000 2638\n",
      "2107500 2638\n",
      "2110000 2638\n",
      "2112500 2638\n",
      "2115000 2638\n",
      "2117500 2638\n",
      "2120000 2638\n",
      "2122500 2638\n",
      "2125000 2638\n",
      "2127500 2638\n",
      "2130000 2638\n",
      "2132500 2638\n",
      "2135000 2638\n",
      "2137500 2638\n",
      "2140000 2638\n",
      "2142500 2638\n",
      "2145000 2638\n",
      "2147500 2638\n",
      "2150000 2638\n",
      "2152500 2638\n",
      "2155000 2638\n",
      "2157500 2638\n",
      "2160000 2638\n",
      "2162500 2638\n",
      "2165000 2638\n",
      "2167500 2638\n",
      "2170000 2638\n",
      "2172500 2638\n",
      "2175000 2638\n",
      "2177500 2638\n",
      "2180000 2638\n",
      "2182500 2638\n",
      "2185000 2638\n",
      "2187500 2638\n",
      "2190000 2638\n",
      "2192500 2638\n",
      "2195000 2638\n",
      "2197500 2638\n",
      "2200000 2638\n",
      "2202500 2638\n",
      "2205000 2638\n",
      "2207500 2638\n",
      "2210000 2638\n",
      "2212500 2638\n",
      "2215000 2638\n",
      "2217500 2638\n",
      "2220000 2638\n",
      "2222500 2638\n",
      "2225000 2638\n",
      "2227500 2638\n",
      "2230000 2638\n",
      "2232500 2638\n",
      "2235000 2638\n",
      "2237500 2638\n",
      "2240000 2638\n",
      "2242500 2638\n",
      "2245000 2638\n",
      "2247500 2638\n",
      "2250000 2638\n",
      "2252500 2638\n",
      "2255000 2638\n",
      "2257500 2638\n",
      "2260000 2638\n",
      "2262500 2638\n",
      "2265000 2638\n",
      "2267500 2638\n",
      "2270000 2638\n",
      "2272500 2638\n",
      "2275000 2638\n",
      "2277500 2638\n",
      "2280000 2638\n",
      "2282500 2638\n",
      "2285000 2638\n",
      "2287500 2638\n",
      "2290000 2638\n",
      "2292500 2638\n",
      "2295000 2638\n",
      "2297500 2638\n",
      "2300000 2638\n",
      "2302500 2638\n",
      "2305000 2638\n",
      "2307500 2638\n",
      "2310000 2638\n",
      "2312500 2638\n",
      "2315000 2638\n",
      "2317500 2638\n",
      "2320000 2638\n",
      "2322500 2638\n",
      "2325000 2638\n",
      "2327500 2638\n",
      "2330000 2638\n",
      "2332500 2638\n",
      "2335000 2638\n",
      "2337500 2638\n",
      "2340000 2638\n",
      "2342500 2638\n",
      "2345000 2638\n",
      "2347500 2638\n",
      "2350000 2638\n",
      "2352500 2638\n",
      "2355000 2638\n",
      "2357500 2638\n",
      "2360000 2638\n",
      "2362500 2638\n",
      "2365000 2638\n",
      "2367500 2638\n",
      "2370000 2638\n",
      "2372500 2638\n",
      "2375000 2638\n",
      "2377500 2638\n",
      "2380000 2638\n",
      "2382500 2638\n",
      "2385000 2638\n",
      "2387500 2638\n",
      "2390000 2638\n",
      "2392500 2638\n",
      "2395000 2638\n",
      "2397500 2638\n",
      "2400000 2638\n",
      "2402500 2638\n",
      "2405000 2638\n",
      "2407500 2638\n",
      "2410000 2638\n",
      "2412500 2638\n",
      "2415000 2638\n",
      "2417500 2638\n",
      "2420000 2638\n",
      "2422500 2638\n",
      "2425000 2638\n",
      "2427500 2638\n",
      "2430000 2638\n",
      "2432500 2638\n",
      "2435000 2638\n",
      "2437500 2638\n",
      "2440000 2638\n",
      "2442500 2638\n",
      "2445000 2638\n",
      "2447500 2638\n",
      "2450000 2638\n",
      "2452500 2638\n",
      "2455000 2638\n",
      "2457500 2638\n",
      "2460000 2638\n",
      "2462500 2638\n",
      "2465000 2638\n",
      "2467500 2638\n",
      "2470000 2638\n",
      "2472500 2638\n",
      "2475000 2638\n",
      "2477500 2638\n",
      "2480000 2638\n",
      "2482500 2638\n",
      "2485000 2638\n",
      "2487500 2638\n",
      "2490000 2638\n",
      "2492500 2638\n",
      "2495000 2638\n",
      "2497500 2638\n",
      "2500000 2638\n",
      "2502500 2638\n",
      "2505000 2638\n",
      "2507500 2638\n",
      "2510000 2638\n",
      "2512500 2638\n",
      "2515000 2638\n",
      "2517500 2638\n",
      "2520000 2638\n",
      "2522500 2638\n",
      "2525000 2638\n",
      "2527500 2638\n",
      "2530000 2638\n",
      "2532500 2638\n",
      "2535000 2638\n",
      "2537500 2638\n",
      "2540000 2638\n",
      "2542500 2638\n",
      "2545000 2638\n",
      "2547500 2638\n",
      "2550000 2638\n",
      "2552500 2638\n",
      "2555000 2638\n",
      "2557500 2638\n",
      "2560000 2638\n",
      "2562500 2638\n",
      "2565000 2638\n",
      "2567500 2638\n",
      "2570000 2638\n",
      "2572500 2638\n",
      "2575000 2638\n",
      "2577500 2638\n",
      "2580000 2638\n",
      "2582500 2638\n",
      "2585000 2638\n",
      "2587500 2638\n",
      "2590000 2638\n",
      "2592500 2638\n",
      "2595000 2638\n",
      "2597500 2638\n",
      "2600000 2638\n",
      "2602500 2638\n",
      "2605000 2638\n",
      "2607500 2638\n",
      "2610000 2638\n",
      "2612500 2638\n",
      "2615000 2638\n",
      "2617500 2638\n",
      "2620000 2638\n",
      "2622500 2638\n",
      "2625000 2638\n",
      "2627500 2638\n",
      "2630000 2638\n",
      "2632500 2638\n",
      "2635000 2638\n",
      "2637500 2638\n",
      "2640000 2638\n",
      "2642500 2638\n",
      "2645000 2638\n",
      "2647500 2638\n",
      "2650000 2638\n",
      "2652500 2638\n",
      "2655000 2638\n",
      "2657500 2638\n",
      "2660000 2638\n",
      "2662500 2638\n",
      "2665000 2638\n",
      "2667500 2638\n",
      "2670000 2638\n",
      "2672500 2638\n",
      "2675000 2638\n",
      "2677500 2638\n",
      "2680000 2638\n",
      "2682500 2638\n",
      "2685000 2638\n",
      "2687500 2638\n",
      "2690000 2638\n",
      "2692500 2638\n",
      "2695000 2638\n",
      "2697500 2638\n",
      "2700000 2638\n",
      "2702500 2638\n",
      "2705000 2638\n",
      "2707500 2638\n",
      "2710000 2638\n",
      "2712500 2638\n",
      "2715000 2638\n",
      "2717500 2638\n",
      "2720000 2638\n",
      "2722500 2638\n",
      "2725000 2638\n",
      "2727500 2638\n",
      "2730000 2638\n",
      "2732500 2638\n",
      "2735000 2638\n",
      "2737500 2638\n",
      "2740000 2638\n",
      "2742500 2638\n",
      "2745000 2638\n",
      "2747500 2638\n",
      "2750000 2638\n",
      "2752500 2638\n",
      "2755000 2638\n",
      "2757500 2638\n",
      "2760000 2638\n",
      "2762500 2638\n",
      "2765000 2638\n",
      "2767500 2638\n",
      "2770000 2638\n",
      "2772500 2638\n",
      "2775000 2638\n",
      "2777500 2638\n",
      "2780000 2638\n",
      "2782500 2638\n",
      "2785000 2638\n",
      "2787500 2638\n",
      "2790000 2638\n",
      "2792500 2638\n",
      "2795000 2638\n",
      "2797500 2638\n",
      "2800000 2638\n",
      "2802500 2638\n",
      "2805000 2638\n",
      "2807500 2638\n",
      "2810000 2638\n",
      "2812500 2638\n",
      "2815000 2638\n",
      "2817500 2638\n",
      "2820000 2638\n",
      "2822500 2638\n",
      "2825000 2638\n",
      "2827500 2638\n",
      "2830000 2638\n",
      "2832500 2638\n",
      "2835000 2638\n",
      "2837500 2638\n",
      "2840000 2638\n",
      "2842500 2638\n",
      "2845000 2638\n",
      "2847500 2638\n",
      "2850000 2638\n",
      "2852500 2638\n",
      "2855000 2638\n",
      "2857500 2638\n",
      "2860000 2638\n",
      "2862500 2638\n",
      "2865000 2638\n",
      "2867500 2638\n",
      "2870000 2638\n",
      "2872500 2638\n",
      "2875000 2638\n",
      "2877500 2638\n",
      "2880000 2638\n",
      "2882500 2638\n",
      "2885000 2638\n",
      "2887500 2638\n",
      "2890000 2638\n",
      "2892500 2638\n",
      "2895000 2638\n",
      "2897500 2638\n",
      "2900000 2638\n",
      "2902500 2638\n",
      "2905000 2638\n",
      "2907500 2638\n",
      "2910000 2638\n",
      "2912500 2638\n",
      "2915000 2638\n",
      "2917500 2638\n",
      "2920000 2638\n",
      "2922500 2638\n",
      "2925000 2638\n",
      "2927500 2638\n",
      "2930000 2638\n",
      "2932500 2638\n",
      "2935000 2638\n",
      "2937500 2638\n",
      "2940000 2638\n",
      "2942500 2638\n",
      "2945000 2638\n",
      "2947500 2638\n",
      "2950000 2638\n",
      "2952500 2638\n",
      "2955000 2638\n",
      "2957500 2638\n",
      "2960000 2638\n",
      "2962500 2638\n",
      "2965000 2638\n",
      "2967500 2638\n",
      "2970000 2638\n",
      "2972500 2638\n",
      "2975000 2638\n",
      "2977500 2638\n",
      "2980000 2638\n",
      "2982500 2638\n",
      "2985000 2638\n",
      "2987500 2638\n",
      "2990000 2638\n",
      "2992500 2638\n",
      "2995000 2638\n",
      "2997500 2638\n",
      "3000000 2638\n",
      "3002500 2638\n",
      "3005000 2638\n",
      "3007500 2638\n",
      "3010000 2638\n",
      "3012500 2638\n",
      "3015000 2638\n",
      "3017500 2638\n",
      "3020000 2638\n",
      "3022500 2638\n",
      "3025000 2638\n",
      "3027500 2638\n",
      "3030000 2638\n",
      "3032500 2638\n",
      "3035000 2638\n",
      "3037500 2638\n",
      "3040000 2638\n",
      "3042500 2638\n",
      "3045000 2638\n",
      "3047500 2638\n",
      "3050000 2638\n",
      "3052500 2638\n",
      "3055000 2638\n",
      "3057500 2638\n",
      "3060000 2638\n",
      "3062500 2638\n",
      "3065000 2638\n",
      "3067500 2638\n",
      "3070000 2638\n",
      "3072500 2638\n",
      "3075000 2638\n",
      "3077500 2638\n",
      "3080000 2638\n",
      "3082500 2638\n",
      "3085000 2638\n",
      "3087500 2638\n",
      "3090000 2638\n",
      "3092500 2638\n",
      "3095000 2638\n",
      "3097500 2638\n",
      "3100000 2638\n",
      "3102500 2638\n",
      "3105000 2638\n",
      "3107500 2638\n",
      "3110000 2638\n",
      "3112500 2638\n",
      "3115000 2638\n",
      "3117500 2638\n",
      "3120000 2638\n",
      "3122500 2638\n",
      "3125000 2638\n",
      "3127500 2638\n",
      "3130000 2638\n",
      "3132500 2638\n",
      "3135000 2638\n",
      "3137500 2638\n",
      "3140000 2638\n",
      "3142500 2638\n",
      "3145000 2638\n",
      "3147500 2638\n",
      "3150000 2638\n",
      "3152500 2638\n",
      "3155000 2638\n",
      "3157500 2638\n",
      "3160000 2638\n",
      "3162500 2638\n",
      "3165000 2638\n",
      "3167500 2638\n",
      "3170000 2638\n",
      "3172500 2638\n",
      "3175000 2638\n",
      "3177500 2638\n",
      "3180000 2638\n",
      "3182500 2638\n",
      "3185000 2638\n",
      "3187500 2638\n",
      "3190000 2638\n",
      "3192500 2638\n",
      "3195000 2638\n",
      "3197500 2638\n",
      "3200000 2638\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def init_process(fin, fout):\n",
    "    outfile = open(fout, 'a')\n",
    "    with open(fin, buffering = 200000, encoding='latin-1') as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                line = line.replace('\"','')\n",
    "                initial_polarity = line.split(',')[0]\n",
    "                if initial_polarity == '0':\n",
    "                    initial_polarity = [1,0]\n",
    "                elif initial_polarity == '4':\n",
    "                    initial_polarity = [0,1]\n",
    "                    \n",
    "                tweet = line.split(',')[-1]\n",
    "                outline = str(initial_polarity)+':::'+tweet\n",
    "                outfile.write(outline)\n",
    "                \n",
    "        except Exception as e:\n",
    "                print(str(e))\n",
    "    outfile.close()\n",
    "    \n",
    "init_process('training.1600000.processed.noemoticon.csv', 'train_set.csv')\n",
    "init_process('testdata.manual.2009.06.14.csv', 'test_set.csv')\n",
    "\n",
    "\n",
    "\n",
    "def create_lexicon(fin):\n",
    "    lexicon = []\n",
    "    with open(fin, 'r', buffering = 100000, encoding='latin-1') as f:\n",
    "        try:\n",
    "            counter = 1\n",
    "            content = ''\n",
    "            for line in f:\n",
    "                counter += 1\n",
    "                if (counter/2500.0).is_integer():\n",
    "                    tweet = line.split(':::')[1]\n",
    "                    content += ' '+tweet\n",
    "                    words = word_tokenize(content)\n",
    "                    words = [lemmatizer.lemmatize(i) for i in words]\n",
    "                    lexicon = list(set(lexicon + words))\n",
    "                    print(counter, len(lexicon))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    with open('lexicon.pickle', 'wb') as f:\n",
    "        pickle.dump(lexicon, f)\n",
    "        \n",
    "create_lexicon('train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def shuffle_data(fin):\n",
    "    df = pd.read_csv(fin, error_bad_lines=False)\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "    print(df.head())\n",
    "    df.to_csv('train_set_shuffled.csv', index=False)\n",
    "    \n",
    "\n",
    "def create_test_data_pickle(fin):\n",
    "    feature_sets = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    with open(fin, buffering=20000) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                features = list(eval(line.split(':::')[0]))\n",
    "                label = list(eval(line.split(':::')[1]))\n",
    "                \n",
    "                feature_sets.append(features)\n",
    "                labels.append(label)\n",
    "                counter += 1\n",
    "            except:\n",
    "                pass\n",
    "        print(counter)\n",
    "        feature_sets = np.array(feature_sets)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "create_test_data_pickle('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "\n",
    "n_classes = 2 \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "total_batches = int(1600000/batch_size)\n",
    "hm_epochs = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([2638, n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "def neural_network_model(data):\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    output = tf.matmul(l2, output_layer['weight']) + output_layer['bias']\n",
    "    return output\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tf_log = 'tf.log'\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    print(prediction)\n",
    "    print('|||||||')\n",
    "    print(y)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer)\n",
    "        \n",
    "        try:\n",
    "            epoch = int(open(tf_log, 'r').read().split('\\n')[-2]) + 1\n",
    "            print('STARTING:', epoch)\n",
    "        except:\n",
    "            epoch = 1\n",
    "            \n",
    "        while epoch <= hm_epochs:\n",
    "            if epoch != 1:\n",
    "                saver.restore(sess,\"model.ckpt\")\n",
    "                epoch_loss = 1\n",
    "                with open('lexicon.pickle', 'rb') as f:\n",
    "                    lexicon = pickle.load(f)\n",
    "                with open('train_set_shuffled.csv', buffering = 200000, encoding='latin-1') as f:\n",
    "                    batch_x = []\n",
    "                    batch_y = []\n",
    "                    batches_run = 0\n",
    "                    for line in f:\n",
    "                        label = line.split(':::')[0]\n",
    "                        tweet = line.split(':::')[1]\n",
    "                        current_words = word_tokenize(tweet.lower())\n",
    "                        current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "                        \n",
    "                        features = np.zeros(len(lexicon))\n",
    "                        \n",
    "                        for word in current_words:\n",
    "                            if word.lower() in lexicon:\n",
    "                                index_vlaue = lexicon.index(word.lower())\n",
    "                                features[index_value] += 1\n",
    "                                line_x = list(features)\n",
    "                                line_y = eval(label)\n",
    "                                batch_x.append(line_x)\n",
    "                                batch_y.append(line_y)\n",
    "                                if len(batch_x) >= batch_size:\n",
    "                                    _, c = sess.run([optimizer, cost], feed_dict={x: np.array(batch_x),\n",
    "                                                                                 y: np.array(batch_y)})\n",
    "                                    epoch_loss += c\n",
    "                                    batch_x = []\n",
    "                                    batch_y = []\n",
    "                                    batches_run += 1\n",
    "                                    print('Batch run:',batches_run, '/', total_batches, '| Epoch:', epoch,'| Batch Loss:', c,)\n",
    "                                    \n",
    "                    saver.save(sess, \"model.ckpt\")\n",
    "                    print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "                    with open(tf_log, 'a') as f:\n",
    "                        f.write(str(epoch) + '\\n')\n",
    "                    epoch += 1\n",
    "                    \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
